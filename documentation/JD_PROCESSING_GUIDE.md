# üìù TruJobs Job Description Processing System Documentation

> **Complete Guide:** Understanding the AI-powered job description processing pipeline, metadata extraction, embedding generation, and search indexing system.

## üìã **Table of Contents**

1. [System Architecture & File Structure](#-system-architecture--file-structure)
2. [Processing Pipeline Overview](#-processing-pipeline-overview)
3. [AI-Powered Content Analysis](#-ai-powered-content-analysis)
4. [Embedding Generation System](#-embedding-generation-system)
5. [Input Methods & Data Handling](#-input-methods--data-handling)
6. [Storage & Search Integration](#-storage--search-integration)
7. [API Configuration & Parameters](#-api-configuration--parameters)
8. [Error Handling & Validation](#-error-handling--validation)
9. [Performance Optimization](#-performance-optimization)
10. [Troubleshooting Guide](#-troubleshooting-guide)

---

## üìÅ **System Architecture & File Structure**

### **Core Job Description Processing Files:**

#### **`lambda_function.py`** - Main Processing Orchestrator
- **Purpose**: Entry point for all job description processing requests
- **Responsibilities**:
  - Handle multiple input types (JSON text, PDF uploads, S3 events)
  - Coordinate the complete JD processing pipeline
  - Generate unique job description IDs and manage file storage
  - Orchestrate AI analysis, embedding generation, and search indexing
  - Manage response formatting and error handling
- **Key Functions**: `lambda_handler()`, main workflow coordination

#### **`ai_service.py`** - AI Processing Core
- **Purpose**: AI-powered job description analysis and embedding generation
- **Responsibilities**:
  - Extract structured metadata using AWS Bedrock Claude model
  - Generate high-quality embeddings using Titan model
  - Parse job requirements, titles, and location information
  - Handle AI model interactions and response processing
  - Manage prompt engineering and response validation
- **Key Functions**: `get_metadata_from_bedrock()`, `get_embedding()`, `get_bedrock_client()`

#### **`search_service.py`** - OpenSearch Integration Layer
- **Purpose**: Handle OpenSearch database operations and indexing
- **Responsibilities**:
  - Initialize authenticated OpenSearch connections
  - Create and manage index mappings for job descriptions
  - Index job description documents with embeddings
  - Handle search index optimization and validation
  - Manage database schema and field mappings
- **Key Functions**: `get_opensearch_client()`, `index_document()`, `create_index()`

#### **`storage_service.py`** - File Storage & Processing
- **Purpose**: Handle file storage operations and PDF processing
- **Responsibilities**:
  - Save text content to S3 as organized files
  - Process and store PDF job descriptions
  - Extract text content from PDF documents using PyPDF2
  - Manage S3 bucket operations and file organization
  - Handle binary data processing and validation
- **Key Functions**: `save_text_to_s3()`, `save_pdf_to_s3()`, `extract_text_from_pdf()`

#### **`input_parser.py`** - Multi-Format Input Handler
- **Purpose**: Parse and validate different input formats
- **Responsibilities**:
  - Detect input type (JSON, multipart form, S3 event)
  - Parse JSON payloads for direct text processing
  - Handle multipart form data for PDF uploads
  - Process S3 event notifications for automated processing
  - Validate input data structure and content
- **Key Functions**: `determine_input_type()`, `parse_json_input()`, `parse_multipart_form()`

#### **`config.py`** - Configuration Management
- **Purpose**: Centralized configuration and environment management
- **Responsibilities**:
  - Define AWS service endpoints and regions
  - Configure Bedrock model IDs and parameters
  - Set S3 bucket names and organizational prefixes
  - Manage OpenSearch connection settings
  - Validate required environment variables
- **Contains**: `CLAUDE_MODEL_ID`, `EMBEDDING_MODEL_ID`, `OPENSEARCH_ENDPOINT`, `BUCKET_NAME`

#### **`prompts.py`** - AI Prompt Engineering
- **Purpose**: Define structured prompts for AI metadata extraction
- **Responsibilities**:
  - Provide optimized prompts for job description analysis
  - Ensure consistent metadata extraction format
  - Define extraction rules for job requirements and qualifications
  - Structure prompts for accurate location and title parsing
- **Key Functions**: `METADATA_EXTRACTION_PROMPT`

#### **`utils.py`** - Utility Functions & Helpers
- **Purpose**: Common utilities and performance monitoring
- **Responsibilities**:
  - Provide timing decorators for performance monitoring
  - Create standardized response formats
  - Implement common helper functions
  - Manage error handling utilities
  - Provide function wrappers with logging
- **Key Functions**: `time_function()`, `success_response()`, `error_response()`

### **Data Flow Architecture:**

```
JD Input ‚Üí input_parser.py ‚Üí storage_service.py ‚Üí AI Processing
    ‚Üì           ‚Üì                    ‚Üì                ‚Üì
Input Detection  Format Parsing    S3 Storage      ai_service.py
    ‚Üì           ‚Üì                    ‚Üì                ‚Üì
Validation   Text/PDF Processing  File Organization  Bedrock Analysis
    ‚Üì           ‚Üì                    ‚Üì                ‚Üì
Processing   Metadata Extraction  Embedding Gen    search_service.py
    ‚Üì           ‚Üì                    ‚Üì                ‚Üì
Response     OpenSearch Indexing  Vector Storage   Database Storage
```

### **Integration Points:**

- **AWS Lambda**: Serverless compute for job description processing
- **AWS Bedrock**: AI models for metadata extraction and embeddings
- **AWS S3**: Document storage and content organization
- **AWS OpenSearch**: Vector database for similarity search and retrieval
- **API Gateway**: HTTP endpoints for various input methods

---

## üîÑ **Processing Pipeline Overview**

### **Complete Job Description Processing Workflow:**

```
1. Input Reception ‚Üí 2. Content Processing ‚Üí 3. AI Analysis ‚Üí 4. Embedding Generation ‚Üí 5. Search Indexing
     ‚Üì                      ‚Üì                   ‚Üì               ‚Üì                      ‚Üì
Multiple Input Types    Text/PDF Extraction   Metadata Extract  Vector Generation     OpenSearch Storage
‚Ä¢ JSON (text)           ‚Ä¢ PDF text parsing    ‚Ä¢ Job title       ‚Ä¢ 1024-dim embedding  ‚Ä¢ Vector indexing
‚Ä¢ Multipart (PDF)       ‚Ä¢ S3 storage         ‚Ä¢ Requirements    ‚Ä¢ Similarity search   ‚Ä¢ Metadata fields
‚Ä¢ S3 Events             ‚Ä¢ Content validation  ‚Ä¢ Location info   ‚Ä¢ Normalization       ‚Ä¢ Search optimization
```

### **Processing Stages in Detail:**

#### **Stage 1: Input Reception & Validation**
- **Input Type Detection**: Automatically identify JSON text, PDF upload, or S3 event
- **Content Validation**: Verify data format, size limits, and required fields
- **Security Checks**: Validate input sources and prevent malicious content
- **ID Generation**: Create unique job description identifiers for tracking

#### **Stage 2: Content Processing & Storage**
- **Text Processing**: Handle direct text input for immediate processing
- **PDF Processing**: Extract text from PDF documents using PyPDF2
- **S3 Storage**: Save original content with organized naming conventions
- **Content Validation**: Ensure text quality and completeness for AI processing

#### **Stage 3: AI-Powered Analysis**
- **Metadata Extraction**: Use Claude model to extract structured job information
- **Requirement Parsing**: Identify skills, qualifications, and experience requirements
- **Location Detection**: Extract and normalize job location information
- **Title Standardization**: Parse and standardize job titles

#### **Stage 4: Embedding Generation**
- **Vector Creation**: Generate 1024-dimensional embeddings using Titan model
- **Quality Assurance**: Validate embedding dimensions and numerical properties
- **Normalization**: Apply proper vector normalization for similarity calculations
- **Performance Optimization**: Efficient embedding generation and caching

#### **Stage 5: Search Indexing & Storage**
- **Index Preparation**: Structure data for OpenSearch compatibility
- **Vector Storage**: Index embeddings with KNN vector configuration
- **Metadata Indexing**: Store structured metadata for filtering and search
- **Search Optimization**: Configure index settings for optimal retrieval performance

---

## üß† **AI-Powered Content Analysis**

### **Bedrock Claude Integration:**

#### **Model Configuration:**
```json
{
  "model_id": "anthropic.claude-3-haiku-20240307-v1:0",
  "anthropic_version": "bedrock-2023-05-31",
  "max_tokens": 4096,
  "temperature": 0.1,
  "analysis_mode": "structured_extraction"
}
```

#### **Metadata Extraction Process:**

##### **1. Job Title Extraction**
- Parse primary job title from various document formats
- Handle variations in title formatting and positioning
- Standardize title format for consistency
- Detect senior/junior level indicators

##### **2. Requirements Analysis**
```python
# Extract structured requirements
job_requirements = [
  "5+ years Python development experience",
  "Bachelor's degree in Computer Science",
  "AWS cloud platform expertise", 
  "Strong problem-solving skills"
]
```

##### **3. Location Processing**
- Extract geographical job location information
- Handle remote work indicators and hybrid arrangements
- Normalize location formats (city, state, country)
- Process multiple location options

#### **Extracted Metadata Structure:**
```json
{
  "job_title": "Senior Software Engineer",
  "job_requirements": [
    "5+ years of Python development experience",
    "Bachelor's degree in Computer Science or related field",
    "Experience with AWS cloud services",
    "Strong problem-solving and analytical skills",
    "Excellent communication and teamwork abilities"
  ],
  "job_location": "San Francisco, CA (Remote Available)"
}
```

#### **Prompt Engineering Strategy:**

##### **Structured Extraction Prompt:**
```python
METADATA_EXTRACTION_PROMPT = """
Analyze the following job description text and extract key metadata in JSON format.
Include only the following fields:
- job_title: The job title
- job_requirements: Array of required skills, qualifications, and experiences  
- job_location: The location of the job

Job description text:
{text}

Return only the JSON object without any additional text or explanation.
Wrap the JSON in <output></output> tags.
"""
```

##### **Extraction Rules:**
- **Literal Extraction**: Extract information exactly as stated
- **No Inference**: Avoid assumptions or interpretations
- **Structured Output**: Ensure consistent JSON format
- **Completeness**: Handle missing information gracefully

---

## üî¢ **Embedding Generation System**

### **Titan Embedding Model Integration:**

#### **Model Configuration:**
```json
{
  "model_id": "amazon.titan-embed-text-v2:0",
  "dimensions": 1024,
  "input_type": "search_document",
  "normalize": true,
  "max_input_length": 8192
}
```

#### **Embedding Generation Process:**

##### **Step 1: Text Preparation**
```python
# Combine metadata into comprehensive text
embedding_text = f"{job_title}\n{' '.join(job_requirements)}\n{job_location}"
```

##### **Step 2: Vector Generation**
```python
def get_embedding(text):
    """Generate 1024-dimensional embedding for job description"""
    response = bedrock_client.invoke_model(
        body=json.dumps({"inputText": text}),
        modelId="amazon.titan-embed-text-v2:0",
        accept="application/json",
        contentType="application/json"
    )
    return response_body['embedding']
```

##### **Step 3: Quality Validation**
- **Dimension Check**: Ensure 1024-dimensional vectors
- **Numerical Validation**: Check for NaN or infinite values
- **Normalization**: Apply L2 normalization for similarity calculations
- **Completeness**: Verify successful embedding generation

#### **Embedding Storage Structure:**
```json
{
  "job_description_id": "uuid-4-format",
  "job_title": "Senior Software Engineer",
  "embedding": [1024 float values],
  "metadata": {
    "job_title": "Senior Software Engineer",
    "job_requirements": ["Python", "AWS", "5+ years"],
    "job_location": "San Francisco, CA",
    "processed_date": "2025-09-17T10:30:00Z"
  },
  "text_content": "Full job description text...",
  "s3_location": "s3://bucket/JD/job-12345.txt"
}
```

---

## üì• **Input Methods & Data Handling**

### **Supported Input Types:**

#### **1. JSON Direct Input** (API Integration)
```json
{
  "text": "Senior Software Engineer position requiring Python expertise...",
  "metadata": {
    "company": "Tech Corp",
    "department": "Engineering",
    "posting_date": "2025-09-17"
  }
}
```

#### **2. Multipart Form Upload** (Web Interface)
```http
POST /job-description-processing
Content-Type: multipart/form-data; boundary=----WebKitFormBoundary

------WebKitFormBoundary
Content-Disposition: form-data; name="file"; filename="job_description.pdf"
Content-Type: application/pdf

[PDF Binary Content]
------WebKitFormBoundary
Content-Disposition: form-data; name="metadata"

{"company": "Tech Corp", "department": "Engineering"}
------WebKitFormBoundary--
```

#### **3. S3 Event Trigger** (Automated Processing)
```json
{
  "Records": [
    {
      "s3": {
        "bucket": {"name": "trujobs-db"},
        "object": {"key": "JD/new-job-description.pdf"}
      }
    }
  ]
}
```

### **Input Processing Features:**

#### **Automatic Format Detection**
- Content-Type header analysis for input classification
- PDF vs text content identification
- S3 event structure recognition
- Fallback mechanisms for ambiguous inputs

#### **Content Validation**
- **Text Quality**: Minimum content length and readability checks
- **PDF Processing**: Valid PDF format and text extraction validation
- **Size Limits**: Configurable maximum content sizes
- **Format Support**: Text and PDF format validation

#### **Error Recovery**
- **Partial Processing**: Continue with available data when possible
- **Format Fallbacks**: Multiple parsing attempts for robustness
- **Graceful Degradation**: Handle missing or corrupted components
- **Comprehensive Logging**: Detailed error tracking and reporting

---

## üîç **Storage & Search Integration**

### **S3 Storage Configuration:**

#### **File Organization Structure:**
```
trujobs-db/
‚îú‚îÄ‚îÄ JD/
‚îÇ   ‚îú‚îÄ‚îÄ {job_id}.txt          # Text job descriptions
‚îÇ   ‚îú‚îÄ‚îÄ {job_id}.pdf          # PDF job descriptions
‚îÇ   ‚îî‚îÄ‚îÄ metadata/
‚îÇ       ‚îî‚îÄ‚îÄ {job_id}_meta.json # Additional metadata
```

#### **Storage Operations:**
```python
# Text storage
s3_key = save_text_to_s3(text_content, f"{job_id}.txt")

# PDF storage with metadata
s3_key = save_pdf_to_s3(pdf_content, f"{job_id}.pdf")
```

### **OpenSearch Index Configuration:**

#### **Index Mapping Structure:**
```json
{
  "mappings": {
    "properties": {
      "job_description_id": {"type": "keyword"},
      "job_title": {"type": "text", "analyzer": "standard"},
      "embedding": {
        "type": "knn_vector",
        "dimension": 1024,
        "method": {
          "name": "hnsw",
          "space_type": "cosinesimil",
          "engine": "lucene"
        }
      },
      "metadata": {
        "type": "object",
        "properties": {
          "job_requirements": {"type": "text"},
          "job_location": {"type": "keyword"},
          "company": {"type": "keyword"},
          "department": {"type": "keyword"}
        }
      },
      "text_content": {"type": "text"},
      "s3_location": {"type": "keyword"},
      "processed_date": {"type": "date"}
    }
  }
}
```

#### **Indexing Process:**

##### **Step 1: Document Preparation**
- Structure job description data for search optimization
- Prepare embedding vectors for KNN similarity search
- Normalize metadata fields for consistent filtering
- Add timestamps and processing metadata

##### **Step 2: Vector Indexing**
- Index 1024-dimensional embedding with HNSW algorithm
- Configure cosine similarity for job-resume matching
- Set up real-time search capabilities
- Optimize index refresh settings

##### **Step 3: Search Optimization**
- Configure field mappings for efficient retrieval
- Set up analyzers for text processing
- Enable faceted search on metadata fields
- Optimize performance for large-scale operations

---

## ‚öôÔ∏è **API Configuration & Parameters**

### **Environment Configuration:**

#### **AWS Services:**
```python
AWS_REGION = 'ap-south-1'
BEDROCK_ENDPOINT = 'https://bedrock-runtime.ap-south-1.amazonaws.com'
OPENSEARCH_ENDPOINT = 'https://xyz.ap-south-1.aoss.amazonaws.com'
```

#### **Model Configuration:**
```python
CLAUDE_MODEL_ID = 'anthropic.claude-3-haiku-20240307-v1:0'
EMBEDDING_MODEL_ID = 'amazon.titan-embed-text-v2:0'
```

#### **Storage Configuration:**
```python
BUCKET_NAME = 'trujobs-db'
JD_PREFIX = 'JD/'
OPENSEARCH_INDEX = 'job_descriptions'
OPENSEARCH_COLLECTION = 'recruitment-search'
```

### **API Response Format:**

#### **Success Response:**
```json
{
  "statusCode": 200,
  "job_description_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "processing_status": "completed",
  "metadata_extracted": true,
  "embedding_generated": true,
  "indexed_successfully": true,
  "processing_time": "2.34s",
  "s3_location": "s3://trujobs-db/JD/job-12345.txt",
  "extracted_metadata": {
    "job_title": "Senior Software Engineer",
    "requirements_count": 8,
    "location": "San Francisco, CA"
  }
}
```

#### **Error Response:**
```json
{
  "statusCode": 400,
  "error": true,
  "message": "PDF processing failed: Invalid file format",
  "error_details": {
    "stage": "content_extraction",
    "original_error": "PyPDF2.PdfReadError: File is not a valid PDF"
  },
  "job_description_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "processing_time": "0.89s"
}
```

---

## üõ°Ô∏è **Error Handling & Validation**

### **Processing Stage Error Handling:**

#### **1. Input Validation Errors**
```python
# Content validation
if not text_content or len(text_content.strip()) < 50:
    raise ValueError('Job description content too short or empty')

# PDF validation  
if content_type == 'application/pdf' and not is_valid_pdf(content):
    raise ValueError('Invalid PDF format or corrupted file')

# Size limits
if len(text_content) > MAX_CONTENT_LENGTH:
    raise ValueError('Job description exceeds maximum length limit')
```

#### **2. AI Processing Errors**
```python
# Bedrock API failures
try:
    metadata = get_metadata_from_bedrock(text)
except Exception as e:
    logger.error(f"Bedrock metadata extraction failed: {str(e)}")
    # Implement fallback processing or return with limited metadata

# Embedding generation errors
try:
    embedding = get_embedding(text)
except Exception as e:
    logger.error(f"Embedding generation failed: {str(e)}")
    # Retry with truncated text or use fallback embedding
```

#### **3. Storage & Indexing Errors**
```python
# S3 storage failures
try:
    s3_key = save_text_to_s3(content, filename)
except Exception as e:
    logger.error(f"S3 storage failed: {str(e)}")
    # Retry with different naming or use temporary storage

# OpenSearch indexing errors
try:
    index_document(opensearch_client, document)
except Exception as e:
    logger.error(f"OpenSearch indexing failed: {str(e)}")
    # Retry indexing or queue for later processing
```

### **Validation Checkpoints:**

#### **Content Validation:**
- **Minimum Length**: Job descriptions must have sufficient content
- **Format Validation**: PDF structure and text extraction quality
- **Character Encoding**: UTF-8 compliance and special character handling
- **Language Detection**: English language content validation

#### **Metadata Validation:**
- **Required Fields**: Job title presence and validity
- **Requirements Format**: Proper array structure for job requirements
- **Location Format**: Valid geographical location information
- **Data Consistency**: Cross-field validation and logical checks

#### **Technical Validation:**
- **Embedding Quality**: Vector dimension and numerical validity
- **Index Compatibility**: OpenSearch mapping compliance
- **Storage Integrity**: S3 upload verification and file accessibility
- **Processing Time**: Performance threshold monitoring

---

## üöÄ **Performance Optimization**

### **Processing Optimizations:**

#### **1. Concurrent Operations**
```python
# Parallel processing of AI operations
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def process_job_description_parallel(text):
    with ThreadPoolExecutor(max_workers=2) as executor:
        # Run metadata extraction and embedding generation concurrently
        metadata_future = executor.submit(get_metadata_from_bedrock, text)
        embedding_future = executor.submit(get_embedding, text)
        
        metadata = metadata_future.result()
        embedding = embedding_future.result()
        
        return metadata, embedding
```

#### **2. Caching Strategies**
```python
# Content-based caching for repeated job descriptions
import hashlib

def get_content_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()

# Cache embeddings for identical content
embedding_cache = {}
def get_cached_embedding(text):
    content_hash = get_content_hash(text)
    if content_hash in embedding_cache:
        return embedding_cache[content_hash]
    
    embedding = get_embedding(text)
    embedding_cache[content_hash] = embedding
    return embedding
```

#### **3. Batch Processing**
- **Multiple JD Processing**: Handle multiple job descriptions in single Lambda invocation
- **Bulk Indexing**: Group multiple documents for efficient OpenSearch indexing
- **S3 Batch Operations**: Optimize file storage with batch uploads
- **Connection Pooling**: Reuse database and service connections

### **Memory Management:**

#### **1. Stream Processing**
```python
# Process large PDFs without loading entire content into memory
def extract_text_streaming(pdf_stream):
    text_chunks = []
    for page in extract_pages_streaming(pdf_stream):
        text_chunks.append(extract_text_from_page(page))
        # Process in chunks to manage memory usage
    return ' '.join(text_chunks)
```

#### **2. Resource Cleanup**
```python
# Ensure proper resource cleanup
def process_with_cleanup(pdf_content):
    temp_files = []
    try:
        # Processing logic
        result = process_job_description(pdf_content)
        return result
    finally:
        # Clean up temporary resources
        for temp_file in temp_files:
            cleanup_temp_file(temp_file)
```

---

## üîß **Troubleshooting Guide**

### **Common Issues & Solutions:**

#### **1. PDF Processing Failures**

**Problem**: `PyPDF2.PdfReadError: File is not a valid PDF`
```python
# Solution: Multi-library PDF processing fallback
def extract_text_with_fallback(pdf_content):
    try:
        return extract_with_pypdf2(pdf_content)
    except PyPDF2.PdfReadError:
        try:
            return extract_with_pdfplumber(pdf_content)
        except Exception:
            return extract_with_pymupdf(pdf_content)
```

**Problem**: Empty or garbled text extraction
```python
# Solution: Enhanced text cleaning and validation
def clean_job_description_text(text):
    # Remove non-printable characters
    text = ''.join(char for char in text if char.isprintable() or char.isspace())
    
    # Normalize whitespace and formatting
    text = ' '.join(text.split())
    
    # Remove common PDF artifacts
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\-.,;:()\[\]{}/"\'@#$%&*+=<>?!]', '', text)
    
    return text.strip()
```

#### **2. AI Model Issues**

**Problem**: Bedrock Claude model rate limiting
```python
# Solution: Exponential backoff retry strategy
import time
import random

def call_bedrock_with_retry(text, max_retries=3):
    for attempt in range(max_retries):
        try:
            return get_metadata_from_bedrock(text)
        except Exception as e:
            if 'ThrottlingException' in str(e) and attempt < max_retries - 1:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                logger.info(f"Rate limited, waiting {wait_time:.2f}s before retry {attempt + 1}")
                time.sleep(wait_time)
            else:
                raise
```

**Problem**: Metadata extraction returning malformed JSON
```python
# Solution: Robust JSON parsing with fallbacks
def parse_metadata_response(response_text):
    try:
        # Try direct JSON parsing
        return json.loads(response_text)
    except json.JSONDecodeError:
        # Extract from output tags
        output_match = re.search(r'<output>(.*?)</output>', response_text, re.DOTALL)
        if output_match:
            try:
                return json.loads(output_match.group(1).strip())
            except json.JSONDecodeError:
                pass
        
        # Fallback: create minimal metadata structure
        return {
            "job_title": extract_title_fallback(response_text),
            "job_requirements": extract_requirements_fallback(response_text),
            "job_location": extract_location_fallback(response_text)
        }
```

#### **3. OpenSearch Issues**

**Problem**: Index mapping conflicts
```python
# Solution: Dynamic index management
def handle_mapping_conflict(client, index_name, document):
    try:
        client.index(index=index_name, body=document)
    except Exception as e:
        if 'mapper_parsing_exception' in str(e):
            # Update mapping or create new index version
            logger.warning(f"Mapping conflict detected, updating index: {str(e)}")
            update_index_mapping(client, index_name, document)
            # Retry indexing
            client.index(index=index_name, body=document)
        else:
            raise
```

**Problem**: Embedding dimension mismatches
```python
# Solution: Vector validation and normalization
def validate_embedding_vector(embedding, expected_dim=1024):
    if not embedding or not isinstance(embedding, list):
        raise ValueError("Invalid embedding: must be a non-empty list")
    
    if len(embedding) != expected_dim:
        if len(embedding) < expected_dim:
            # Pad with zeros
            embedding.extend([0.0] * (expected_dim - len(embedding)))
            logger.warning(f"Padded embedding from {len(embedding)} to {expected_dim} dimensions")
        else:
            # Truncate to expected dimension
            embedding = embedding[:expected_dim]
            logger.warning(f"Truncated embedding from {len(embedding)} to {expected_dim} dimensions")
    
    # Validate numerical values
    if any(not isinstance(x, (int, float)) or math.isnan(x) or math.isinf(x) for x in embedding):
        raise ValueError("Embedding contains invalid numerical values")
    
    return embedding
```

### **Performance Troubleshooting:**

#### **1. Slow Processing Times**

**Diagnostic Approach:**
```python
# Add comprehensive timing measurements
class ProcessingTimer:
    def __init__(self):
        self.stages = {}
    
    def time_stage(self, stage_name):
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    duration = time.time() - start_time
                    self.stages[stage_name] = duration
                    logger.info(f"{stage_name} completed in {duration:.2f}s")
                    return result
                except Exception as e:
                    duration = time.time() - start_time
                    logger.error(f"{stage_name} failed after {duration:.2f}s: {str(e)}")
                    raise
            return wrapper
        return decorator

# Usage example
timer = ProcessingTimer()

@timer.time_stage("pdf_extraction")
def extract_text_timed(pdf_content):
    return extract_text_from_pdf(pdf_content)
```

**Common Performance Bottlenecks:**
- **PDF Processing**: Large files or complex layouts
- **AI Model Calls**: Network latency and model processing time
- **Embedding Generation**: Large text content or model availability
- **Database Operations**: Index size or connection issues

#### **2. High Error Rates**

**Error Monitoring System:**
```python
from collections import defaultdict
import datetime

class ErrorTracker:
    def __init__(self):
        self.error_counts = defaultdict(int)
        self.error_details = []
    
    def track_error(self, error_type, error_message, context=None):
        self.error_counts[error_type] += 1
        self.error_details.append({
            'type': error_type,
            'message': error_message,
            'context': context,
            'timestamp': datetime.datetime.utcnow().isoformat()
        })
        
        # Alert on high error rates
        if self.error_counts[error_type] > ERROR_THRESHOLD:
            self.send_alert(f"High error rate for {error_type}: {self.error_counts[error_type]} errors")

error_tracker = ErrorTracker()
```

### **Debugging Tools:**

#### **1. Enhanced Logging Configuration**
```python
import logging
import sys

def setup_logging():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('job_processing.log', mode='a')
        ]
    )
    
    # Set specific log levels for different components
    logging.getLogger('boto3').setLevel(logging.WARNING)
    logging.getLogger('botocore').setLevel(logging.WARNING)
    logging.getLogger('opensearch').setLevel(logging.INFO)
```

#### **2. Request Correlation and Tracing**
```python
import uuid
import contextvars

# Create context variable for request correlation
correlation_id = contextvars.ContextVar('correlation_id')

def process_job_description_with_tracing(event, context):
    # Generate correlation ID for request tracking
    req_id = str(uuid.uuid4())
    correlation_id.set(req_id)
    
    logger = logging.getLogger(__name__)
    logger.info(f"[{req_id}] Starting job description processing")
    
    try:
        result = process_job_description(event)
        logger.info(f"[{req_id}] Processing completed successfully")
        return result
    except Exception as e:
        logger.error(f"[{req_id}] Processing failed: {str(e)}", exc_info=True)
        raise
```

---

## üìä **Monitoring & Metrics**

### **Key Performance Indicators:**

#### **Processing Metrics:**
- **Throughput**: Job descriptions processed per hour/minute
- **Success Rate**: Percentage of successful processing operations
- **Average Processing Time**: End-to-end processing duration
- **Error Rate**: Percentage of failed processings by stage

#### **Quality Metrics:**
- **Metadata Extraction Accuracy**: Successful field extraction rate
- **Embedding Generation Success**: Successful vector creation rate
- **Indexing Success**: Successful OpenSearch indexing rate
- **Content Quality**: Readable text extraction percentage

#### **Resource Utilization:**
- **Lambda Duration**: Function execution time distribution
- **Memory Usage**: Peak memory consumption per processing
- **API Call Costs**: Bedrock model invocation costs
- **Storage Costs**: S3 storage and OpenSearch index costs

### **Alerting Configuration:**

#### **Critical Alerts:**
- Processing success rate < 90%
- Average processing time > 60 seconds
- Error rate > 10% for any processing stage
- Embedding generation failures > 5%

#### **Warning Alerts:**
- Processing time > 30 seconds
- Memory usage > 75% of allocated
- API call rate approaching service limits
- Storage costs exceeding budget thresholds

---

**üéØ Ready to Process Job Descriptions!** This comprehensive guide covers all aspects of the TruJobs job description processing system, from initial content upload to final search indexing for candidate matching.